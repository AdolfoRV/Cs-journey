{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo LSA sobre descriptores TF-IDF\n",
    "\n",
    "**Curso**: CC5213 - Recuperación de Información Multimedia  \n",
    "**Profesor**: Juan Manuel Barrios  \n",
    "**Fecha**: 21 de mayo de 2025\n",
    "\n",
    "\n",
    "Primero se usará el mismo ejemplo  de la semana anterior de crear descriptores tf-idf y calcular la similitud coseno entre todos.\n",
    "\n",
    "Luego se reducirá la dimensión de los vectores con LSA usando dos métodos:\n",
    " 1. Factorizar las matrices con un método iterativo aproximado (más rápido)\n",
    " 2. Factorizar directamente las matrices (más lento)\n",
    "\n",
    "Para cada uno se compara el resultado obtenido por LSA versus el obtenido por los descriptores originales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcular similitudes con tf-idf (igual a semana anterior)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy\n",
    "import os\n",
    "\n",
    "class File():\n",
    "    def __init__(self):\n",
    "        self.filename = \"\"\n",
    "        self.titulo = \"\"\n",
    "        self.autor = \"\"\n",
    "        self.fecha = \"\"\n",
    "        self.tipo = \"\"\n",
    "        self.url = \"\"\n",
    "        self.full_text = \"\"\n",
    "\n",
    "    def load_full_text(self, dir_dataset):\n",
    "        file = os.path.join(dir_dataset, self.filename)\n",
    "        with open(file, mode=\"r\", encoding=\"utf8\") as f:\n",
    "            self.full_text = f.read()\n",
    "        # quitar la primera linea (es una url)\n",
    "        self.full_text = self.full_text[self.full_text.find(\"\\n\") + 1 :]\n",
    "  \n",
    "\n",
    "def load_dataset(dir_dataset):\n",
    "    files = list()\n",
    "    header = dict()\n",
    "    file_dataset = os.path.join(dir_dataset, 'dataset.txt')\n",
    "    print(\"leyendo {}...\".format(file_dataset))\n",
    "    with open(file_dataset, mode=\"r\", encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            fields = line.strip().split(\"\\t\")\n",
    "            # leer los nombres de la primera fila \n",
    "            if len(header) == 0:\n",
    "                header = dict((c, i) for i, c in enumerate(fields))\n",
    "                continue\n",
    "            file = File()\n",
    "            file.filename = fields[header[\"Filename\"]]\n",
    "            file.titulo = fields[header[\"Titulo\"]]\n",
    "            file.autor = fields[header[\"Por\"]]\n",
    "            file.fecha = fields[header[\"Fecha\"]]\n",
    "            file.tipo = fields[header[\"Tipo_documento\"]]\n",
    "            file.url = fields[header[\"URL\"]]\n",
    "            files.append(file)\n",
    "    dir_dataset = os.path.join(dir_dataset, 'dataset')\n",
    "    print(\"leyendo textos de {} archivos en {} ...\".format(len(files), dir_dataset))\n",
    "    for file in files:\n",
    "        file.load_full_text(dir_dataset)\n",
    "    return files\n",
    "\n",
    "t1 = time.time()\n",
    "dataset_files = load_dataset('dataset_ciperchile.21-05-2025')\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"leídos {} documentos en {:.1f} segundos\".format(len(dataset_files), t2-t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opcional, restringir los documentos a usar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se podrían usar menos documentos, para que no sea tan lento\n",
    "cantidad_documentos = 5174  #len(dataset_files)\n",
    "\n",
    "dataset_files = dataset_files[0:cantidad_documentos]\n",
    "\n",
    "print(\"Usando {} documentos\".format(len(dataset_files)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcular vocabulario (igual a ejemplo semana anterior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#un array con los textos para crear el vocabulario y descriptores\n",
    "textos = list()\n",
    "largo_total = 0\n",
    "for doc in dataset_files:\n",
    "    largo_total += len(doc.full_text)\n",
    "    textos.append(doc.full_text)\n",
    "\n",
    "print(\"Se van a usar {} documentos, con largo total {:.1f} MB\".format(len(textos), largo_total/1024/1024))\n",
    "\n",
    "#calcular el vocabulario: total de palabras a representar\n",
    "t0 = time.time()\n",
    "vectorizer = TfidfVectorizer(lowercase=True,          # por defecto es True \n",
    "                             strip_accents='unicode', # por defecto es None. unicode=eliminar acentos\n",
    "                             sublinear_tf=True,       # por defecto es False. TRUE=usar 1+log(freq) \n",
    "                             use_idf=True,            # por defecto es True\n",
    "                             norm='l2',               # por defecto es l2 (asi el coseno es solo la multiplicacion de valores)\n",
    "                             ngram_range=(1,1),       # por defecto es (1,1). rango de ngramas a usar por ej: (1,1) o (1,2) o (1,3)\n",
    "                             max_df=1.0, # Si una palabra aparece en más que max_df documentos, se ignora (probar con 0.9)\n",
    "                                         #  Si es float -> porcentaje del total de documentos\n",
    "                                         #  Si es int   -> cantidad de documentos\n",
    "                                         #  Notar que es distinto 1.0 (el 100% de los documentos) vs 1 (solo 1 documento)\n",
    "                             min_df=0.0  # Si una palabra aparece en menos que min_df documentos, se ignora (probar con 0.1)\n",
    "                             )           #  También puede ser float o int\n",
    "vectorizer.fit(textos)\n",
    "t1 = time.time()\n",
    "\n",
    "print(\"tiempo para crear vocabulario: {:.1f} segs\".format(t1-t0))\n",
    "print(\"vocabulario de {} palabras\".format(len(vectorizer.vocabulary_)))\n",
    "print(\"primeras 10 palabras: \", list(vectorizer.vocabulary_.keys())[0:10])\n",
    "print(\"primeras 10 palabras ordenadas: \", list(sorted(vectorizer.vocabulary_.keys())[0:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcular descriptores (igual a ejemplo semana anterior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform() entrega el descriptor tf-idf (un vector sparse con los pesos de las palabras)\n",
    "t0 = time.time()\n",
    "descriptores = vectorizer.transform(textos)\n",
    "t1 = time.time()\n",
    "\n",
    "print(\"tiempo descriptores: {:.1f} segs\".format(t1-t0))\n",
    "print(\"descriptores es matriz de {}\".format(descriptores.shape))\n",
    "\n",
    "#verificar que los descriptores es una matriz sparse\n",
    "print(\"descriptores tipo {}\".format(type(descriptores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcular similares (igual a ejemplo semana anterior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#usar la funcion toarray() para convertir los descriptores a una matriz densa\n",
    "#multiplicar las matrices con matmul() para calcular la similitud coseno\n",
    "\n",
    "t0 = time.time()\n",
    "descriptores_denso = descriptores.toarray()\n",
    "auto_similitud = numpy.matmul(descriptores_denso, descriptores_denso.T)\n",
    "t1 = time.time()\n",
    "\n",
    "print(\"Tiempo comparación todos contra todos: {:.1f} segs\".format(t1-t0))\n",
    "\n",
    "print(auto_similitud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imprimir el resultado de similitud coseno (igual a ejemplo semana anterior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "def imprimir_similares(matriz, min_score):\n",
    "    #obtener el maximo por fila\n",
    "    #primero llenar la diagonal con ceros para que el mas parecido no sea si mismo\n",
    "    numpy.fill_diagonal(matriz, 0)\n",
    "    \n",
    "    filas = []\n",
    "    for i in range(matriz.shape[0]):\n",
    "        #obtener la posicion del maximo por fila\n",
    "        posicion_mayor = numpy.argmax(matriz[i])\n",
    "        #nombres de archivos\n",
    "        nombre_documento = dataset_files[i].filename\n",
    "        nombre_mas_similar = dataset_files[posicion_mayor].filename\n",
    "        similitud = matriz[i, posicion_mayor]\n",
    "        #imprimir documentos con score mayor al mínimo pedido\n",
    "        if similitud >= min_score:\n",
    "            fila = {'id':  i,\n",
    "                    'query':  nombre_documento,\n",
    "                    'documento': nombre_mas_similar,\n",
    "                    'similitud coseno': similitud}\n",
    "            filas.append(fila)\n",
    "    \n",
    "    print(\"documentos con similitud mayor a {}:\".format(min_score))\n",
    "    print()\n",
    "    df = pandas.DataFrame(filas)\n",
    "    print(df.to_string(index=False, justify='center'))\n",
    "\n",
    "imprimir_similares(auto_similitud, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Semantic Analysis (LSA)\n",
    "\n",
    "LSA consiste en reducir la cantidad de términos del vocabulario, agrupando los términos que aparecen correlacionados. Conceptualmente es muy similar a Análisis de Componentes Principales (PCA) que veremos más adelante en el curso.\n",
    "\n",
    "A continuación se define la cantidad de conceptos que se van a calcular para reducir de dimensionalidad los vectores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA opción 1: usar método aproximado de Scikit-Learn\n",
    "\n",
    "El algoritmo de TruncatedSVD aproxima la descomposición con un método iterativo (demora como 1 minuto).   \n",
    "\n",
    " * `n_components` define la nueva dimensionalidad de los vectores\n",
    " * `n_iter` es la cantidad de iteraciones del método aproximado. Al aumentarlo mejora la aproximación al resultado real aunque toma más tiempo de calcular.\n",
    "\n",
    "Ver documentacion en: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# numero de dimensiones a los que se reducirán los descriptores\n",
    "# con un numero pequeño funciona todo muy rápido aunque no muy buena calidad\n",
    "num_conceptos = 50\n",
    "\n",
    "t0 = time.time()\n",
    "transformer_tsvd = TruncatedSVD(n_components=num_conceptos, n_iter=20, random_state=4)\n",
    "transformer_tsvd.fit(descriptores)\n",
    "t1 = time.time()\n",
    "\n",
    "print(\"tiempo ajustar la transformacion: {:.1f} segs\".format(t1-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revisar el espacio reducido y valores singulares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se imprimen los valores singulares y su suma total con respecto al total\n",
    "print(\"matriz proyeccion conceptos latentes={}\".format(transformer_tsvd.components_.shape))\n",
    "print(\"primer concepto latente={}\".format(transformer_tsvd.components_[0]))\n",
    "print(\"segundo concepto latente={}\".format(transformer_tsvd.components_[1]))\n",
    "print(\"tercer concepto latente={}\".format(transformer_tsvd.components_[2]))\n",
    "print(\"PORCENTAJE DE VARIANZA={:.1f}%\".format(transformer_tsvd.explained_variance_ratio_.sum()*100))\n",
    "print(transformer_tsvd.singular_values_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mostrar los primeros conceptos latentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imprimir_conceptos_tsvd(conceptos_a_mostrar, palabras_por_concepto):\n",
    "    vocabulario = numpy.array(vectorizer.get_feature_names_out())\n",
    "    for i, pesos in enumerate(transformer_tsvd.components_):\n",
    "        if i >= conceptos_a_mostrar:\n",
    "            break\n",
    "        indices_menor_a_mayor = numpy.argsort(pesos)\n",
    "        indices_mayor_a_menor = indices_menor_a_mayor[::-1]\n",
    "        pesos_mayor_a_menor = pesos[indices_mayor_a_menor]\n",
    "        terminos_mayor_a_menor = vocabulario[indices_mayor_a_menor]\n",
    "        lista=[]\n",
    "        for j in range(palabras_por_concepto):\n",
    "            lista.append(\"{}({:.2f})\".format(terminos_mayor_a_menor[j],pesos_mayor_a_menor[j]))\n",
    "        print(\"CONCEPTO LATENTE #{:2d}: {}\".format(i, \" \".join(lista)))\n",
    "\n",
    "imprimir_conceptos_tsvd(10, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducir de dimensión los descriptores tf-idf\n",
    "Se obtiene la matriz de descriptores de dimension dada en el parámetro `n_components`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptores_tsvd = transformer_tsvd.transform(descriptores)\n",
    "print(\"{} -> {} \".format(descriptores.shape, descriptores_tsvd.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcular la similitud en el espacio reducido\n",
    "\n",
    "Como los descriptores tienen menos dimensiones, el tiempo de comparación es mucho menor al usado con los descriptores originales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "similitudes_tsvd = numpy.matmul(descriptores_tsvd, descriptores_tsvd.T)\n",
    "t1 = time.time()\n",
    "print(\"tiempo comparacion todos contra todos: {:.1f} segs\".format(t1-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparar con lo obtenido con los descriptores originales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "def evaluar_similitud(matriz_similitud, conceptos, matriz_ideal, imprimir=True):\n",
    "    #completo la diagonal con cero para que el mas parecido no sea si mismo\n",
    "    numpy.fill_diagonal(matriz_similitud, 0)\n",
    "    numpy.fill_diagonal(matriz_ideal, 0)\n",
    "\n",
    "    #obtener la posicion del maximo por fila\n",
    "    posicion_max = numpy.argmax(matriz_similitud, axis=1)\n",
    "    mayor = numpy.amax(matriz_similitud, axis=1)\n",
    "    \n",
    "    #posicion del mas similar por fila\n",
    "    posicion_max_ideal = numpy.argmax(matriz_ideal, axis=1)\n",
    "\n",
    "    #imprimir los documentos mas parecidos y su similitud\n",
    "    filas = []\n",
    "    total = matriz_ideal.shape[0]\n",
    "    total_iguales = 0\n",
    "    for i in range(total):\n",
    "        nombre_documento = dataset_files[i].filename\n",
    "        nombre_mas_similar = dataset_files[posicion_max[i]].filename\n",
    "        nombre_ideal = dataset_files[posicion_max_ideal[i]].filename\n",
    "        resultado = \"\"\n",
    "        if posicion_max[i] == posicion_max_ideal[i]:\n",
    "            resultado = \"¡igual! :-) ok\"\n",
    "            total_iguales += 1\n",
    "        else:\n",
    "            resultado = \"¡distinto! era {}\".format(nombre_ideal)\n",
    "        filas.append([nombre_documento, nombre_mas_similar, mayor[i], resultado])\n",
    "    pct_coincidencias = total_iguales / total * 100\n",
    "    if imprimir:\n",
    "        print(\"mostrando resultados al usar {} conceptos\".format(conceptos))\n",
    "        print(\"iguales al original: {} / {} = {:.1f}%\".format(total_iguales, total, pct_coincidencias))\n",
    "        print()\n",
    "        df = pandas.DataFrame(filas, columns=[\"query\", \"similar\", \"similitud\", \"resultado\"])\n",
    "        print(df.to_string(index=False,justify='center'))\n",
    "    return pct_coincidencias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluar_similitud(similitudes_tsvd, descriptores_tsvd.shape[1], auto_similitud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA opción 2: Usar método exacto de factorización de matrices\n",
    "\n",
    "Usaremos la implementación de Numpy para la SVD (Singular Value Decomposition).  \n",
    "Ver documentacion en: https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html\n",
    "\n",
    "Es mucho más lento que el método anterior ya que es exacto. La ventaja es que es independiente del número de dimensiones a proyectar, por lo que se calcula una vez y luego se puede probar con distinta cantidad de conceptos latentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import svd\n",
    "\n",
    "#demora unos 5 minutos para todo el dataset\n",
    "t0 = time.time()\n",
    "descriptores_denso = descriptores.toarray()\n",
    "U, S, VT = numpy.linalg.svd(descriptores_denso, full_matrices=False, compute_uv=True)\n",
    "t1 = time.time()\n",
    "print(\"tiempo factorizacion USV: {:.1f} segs ({:.1f} mins)\".format(t1-t0,(t1-t0)/60))\n",
    "\n",
    "#notar que S se guarda como un array en vez de matriz\n",
    "#con numpy.diag(S) obtenemos la matriz diagonal\n",
    "print(\"A{} = U{} x S{} x V^T{}\".format(descriptores_denso.shape, U.shape, S.shape, VT.shape))\n",
    "print(\"array S: {} {} {} ... {} {}\".format(S[0], S[1], S[3], S[-2], S[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seleccionar dimensiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# numero de dimensiones a los que se reducirán los descriptores\n",
    "# notar que ya se realizó el cálculo de la factorización, por lo que ahora es directo probar con varios numeros\n",
    "cantidad_conceptos = 50\n",
    "\n",
    "#probar con distinto numero de conceptos (es rapido, no requiere volver a factorizar los descriptores)\n",
    "Uk = U[:, :cantidad_conceptos]\n",
    "Sk = S[0:cantidad_conceptos]\n",
    "VkT = VT[:cantidad_conceptos, :]\n",
    "\n",
    "print(\"Uk{} x Sk{} x Vk^T{}\".format(Uk.shape, Sk.shape, VkT.shape))\n",
    "\n",
    "#se imprimen los valores singulares y su suma total con respecto al total\n",
    "print(\"PORCENTAJE DE VARIANZA={:.1f}%\".format(numpy.sum(Sk)/numpy.sum(S)*100))\n",
    "print(Sk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imprimir_conceptos_svd(conceptos_a_mostrar, palabras_por_concepto):\n",
    "    vocabulario = numpy.array(vectorizer.get_feature_names_out())\n",
    "    for fila in range(conceptos_a_mostrar):\n",
    "        pesos = numpy.abs(numpy.array(VkT[fila])) # pueden salir pesos negativos\n",
    "        indices_menor_a_mayor = numpy.argsort(pesos)\n",
    "        indices_mayor_a_menor = indices_menor_a_mayor[::-1]\n",
    "        pesos_mayor_a_menor = pesos[indices_mayor_a_menor]\n",
    "        terminos_mayor_a_menor = vocabulario[indices_mayor_a_menor]\n",
    "        lista=[]\n",
    "        for j in range(palabras_por_concepto):\n",
    "            lista.append(\"{}({:.2f})\".format(terminos_mayor_a_menor[j],pesos_mayor_a_menor[j]))\n",
    "        print(\"CONCEPTO LATENTE #{:2d}: {}\".format(fila, \" \".join(lista)))\n",
    "\n",
    "imprimir_conceptos_svd(10, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducir de dimensión los descriptores originales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#METODO 1: reducir dimensiones de los descriptores \n",
    "Ak = numpy.matmul(descriptores_denso, VkT.T)\n",
    "print(\"Ak={}\".format(Ak.shape))\n",
    "\n",
    "#METODO 2: alternativamente podemos reducirlos multiplicando: U*S\n",
    "Ak_alt = numpy.matmul(Uk, numpy.diag(Sk))\n",
    "print(\"Ak_alt={}\".format(Ak_alt.shape))\n",
    "\n",
    "#comprobando la diferencia entre ambos metodos\n",
    "diferencias = numpy.subtract(Ak, Ak_alt)\n",
    "print(\"diferencias entre Ak - Ak_alt: max={} min={}\".format(numpy.max(diferencias), numpy.min(diferencias)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcular la similitud de los descriptores reducidos y comparar resultados con descriptores originales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#se realiza la búsqueda igual que en el ejemplo anterior\n",
    "t0 = time.time()\n",
    "similitudes_svd= numpy.matmul(Ak, Ak.T)\n",
    "t1 = time.time()\n",
    "print(\"tiempo comparacion todos contra todos: {:.1f} segs\".format(t1-t0))\n",
    "\n",
    "evaluar_similitud(similitudes_svd, Ak.shape[1], auto_similitud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicios Propuestos\n",
    "\n",
    "__1.__ Hacer un gráfico que muestre cómo auymenta la calidad de la respuesta al aumentar la cantidad de conceptos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacer un ciclo con distintos número de conceptos. Para cada uno:\n",
    "#  -Proyectar U, S y V a Uk, Sk y Vk (tomando las primeras columans)\n",
    "#  -Reducir los descriptores originales (descriptores_denso) a Ak  (multiplicar matrices)\n",
    "#  -Comparar los descriptores y encontrar la matriz de similityd\n",
    "#  -Comparar con la matriz original llamando a evaluar_similitud()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2.__ ¿Cómo buscar documentos similares cuando el documento de consulta no se usó para calcular LSA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Calcular el documento más similar para los siguientes documentos de consulta.\n",
    "textos_q = [ \"elecciones de alcalde\", \"estudiantes de universidad\"]\n",
    "\n",
    "#1.calcular la matriz de descriptores para textos_q (usar el vocabulario ya calculado)\n",
    "#2.proyectar la matriz de descriptores al espacio reducido\n",
    "#3.buscar el documento más similar en el espacio reducido\n",
    "#4.comprobar si se obtiene el mismo resultado en el espacio original\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "prueba.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
